{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c077d4-2ea8-479c-8acc-72d93d7b45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca85922-8f26-438e-9cc8-8730b05a5c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from hydra import initialize, compose\n",
    "import plotly.express as px\n",
    "\n",
    "with initialize(config_path=\"../run/conf\", version_base=None):\n",
    "    cfg = compose(\"cv_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178eb81c-0c50-4ed7-aca5-1a83b89c8f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utils.metrics import event_detection_ap\n",
    "from src.utils.periodicity import get_periodicity_dict\n",
    "from src.utils.post_process import make_submission\n",
    "from src.utils.common import trace\n",
    "\n",
    "periodicity_dict = get_periodicity_dict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edce40af-66f6-47bc-b60d-0ef85c52ad48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_df = pl.read_csv(Path(cfg.dir.data_dir) / \"train_events.csv\").drop_nulls()\n",
    "event_df = event_df.with_columns(\n",
    "    pl.col(\"timestamp\").str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d9dd66-a605-441c-b836-16aca34d8d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df = (\n",
    "    pl.read_parquet(\"./pred_onset.parquet\")\n",
    "    .rename({\"label_pred\": \"stacking_prediction_onset\"})\n",
    "    .drop(\"label\")\n",
    "    .join(\n",
    "        pl.read_parquet(\"./pred_wakeup.parquet\")\n",
    "        .rename({\"label_pred\": \"stacking_prediction_wakeup\"})\n",
    "        .drop(\"label\"),\n",
    "        on=[\"series_id\", \"step\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "pred_df = pred_df.with_columns(\n",
    "    ((pl.col(\"step\") - pl.col(\"step\").shift(1)) != 12)\n",
    "    .cast(int)\n",
    "    .cumsum()\n",
    "    .over(\"series_id\")\n",
    "    .fill_null(0)\n",
    "    .alias(\"chunk_id\")\n",
    ").with_columns(pl.col('step').cast(pl.UInt32))\n",
    "\n",
    "train_df = pl.read_parquet(Path(cfg.dir.data_dir) / \"train_series.parquet\")\n",
    "train_df = train_df.with_columns(\n",
    "            pl.col(\"timestamp\").str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        ).filter(pl.col('step')%12==0)\n",
    "pred_df = pred_df.join(train_df, on=['series_id', 'step'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30340c-73dc-463f-8658-e45fb26a09eb",
   "metadata": {},
   "source": [
    "## day ごとにevent検出をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99b22404-542a-4a50-8cd7-6b77c78c4f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "def post_process_from_2nd(\n",
    "    pred_df,\n",
    "    event_rate: int | float = 50,\n",
    "    height: float = 0.001,\n",
    "    event2col: dict[str, str] = {\"onset\": \"stacking_prediction_onset\", \"wakeup\": \"stacking_prediction_wakeup\"},\n",
    "    event2offset: dict[str, str] = {\"onset\": \"5h\", \"wakeup\": \"0h\"},\n",
    "    use_daily_norm: bool = True,\n",
    "    daily_score_offset=10,\n",
    "    later_date_max_sub_rate: float | None = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    1分ごとの予測値を用いてイベントを検出する\n",
    "    用語\n",
    "    - 予測地点: 2段目のモデルによって得られた1分毎の予測位置\n",
    "    - 候補地点: event の候補となる 15秒 or 45秒始まりの30秒間隔の位置\n",
    "\n",
    "    Args:\n",
    "        pred_df (pl.DataFrame): timestamp 込み\n",
    "        event_rate (int | float, optional): [0,1) の値であれば1分間に何回イベントが起こるか。intの場合はseries_idごとに同じイベント数を検出。 Defaults to 0.005.\n",
    "        height (float, optional): 候補地点の期待値がこの値を下回ったら終了。 Defaults to 0.1.\n",
    "        event2col (dict[str, str], optional): event名と予測値のカラム名の対応。 Defaults to {\"onset\": \"stacking_prediction_onset\", \"wakeup\": \"stacking_prediction_wakeup\"}.\n",
    "        weight_rate (float | None, optional): 遠くの予測値の期待値を割り引く際の重み。Noneの場合は重みを1とする。1/weight_rate 倍ずつ遠くの予測値の重みが小さくなっていく。 Defaults to None.\n",
    "        use_daily_norm (bool, optional): 一日ごとに予測値を正規化するかどうか。 Defaults to False.\n",
    "        daily_score_offset (float, optional): 正規化の際のoffset。 Defaults to 1.0.\n",
    "        later_date_max_sub_rate (float | None, optional): 日付が古いほど予測値を割り引く際の最大割引率。Noneの場合は割引しない。 Defaults to None.\n",
    "    Returns:\n",
    "        event_df (pl.DataFrame): row_id, series_id, step, event, score をカラムに持つ。\n",
    "    \"\"\"\n",
    "    high_match_nums = (0, 1, 3, 5, 8, 10, 13, 15, 20, 25, 30)\n",
    "    low_match_nums = (0, 1, 3, 5, 7, 10, 12, 15, 20, 25, 30)\n",
    "    match_sums = np.ones(10)\n",
    "    result_events_records = []\n",
    "\n",
    "    # event ごとに処理\n",
    "    for event, event_pred_col in event2col.items():\n",
    "        \"\"\"\n",
    "        元の系列の予測地点(長さN): 0, 12, 24, 36, 48, 60, 72, 84, 96, 108, ..., (N-1)*12\n",
    "        15秒から30秒おきのevent候補地点(長さ2N): 3, 9, 15, 21, 27, 33, 39, 45, 51, 57, ..., (N-1)*12+3, (N-1)*12+9\n",
    "            - 15秒(3step)から1分おき(長さN): 3, 15, 27, 39, 51, 63, 75, 87, 99, 111, ..., (N-1)*12+3\n",
    "                - 左の個数 {12: 1, 36: 3, 60: 5, 90: *8*, 120: 10, 150: *13*, 180: 15, 240: 20, 300: 25, 360: 30} high_match_nums\n",
    "                - 右の個数 {12: 1, 36: 3, 60: 5, 90: *7*, 120: 10, 150: *12*, 180: 15, 240: 20, 300: 25, 360: 30} low_match_nums\n",
    "            - 45秒(9step)から1分おき(長さN): 9, 21, 33, 45, 57, 69, 81, 93, 105, 117, ..., (N-1)*12+9\n",
    "                - 左の個数 {12: 1, 36: 3, 60: 5, 90: *7*, 120: 10, 150: *12*, 180: 15, 240: 20, 300: 25, 360: 30} low_match_nums\n",
    "                - 右の個数 {12: 1, 36: 3, 60: 5, 90: *8*, 120: 10, 150: *13*, 180: 15, 240: 20, 300: 25, 360: 30} high_match_nums       \n",
    "        \"\"\"\n",
    "\n",
    "        # series内でのindexを振り、chunk内での最大と最小を計算\n",
    "        minute_pred_df = pred_df.filter(pl.col(\"timestamp\").is_not_null())\n",
    "\n",
    "        if use_daily_norm:\n",
    "            minute_pred_df = (\n",
    "                minute_pred_df.with_columns(\n",
    "                    pl.col(\"timestamp\").dt.offset_by(event2offset[event]).dt.date().alias(\"date\")\n",
    "                )\n",
    "                .with_columns(pl.col(event_pred_col).sum().over([\"series_id\", \"date\"]).alias(\"date_sum\"))\n",
    "                .with_columns(\n",
    "                    pl.col(event_pred_col) / (pl.col(\"date_sum\") + (1 / (daily_score_offset + pl.col(\"date_sum\"))))\n",
    "                )\n",
    "            )\n",
    "        if later_date_max_sub_rate is not None:\n",
    "            minute_pred_df = minute_pred_df.with_columns(\n",
    "                pl.col(\"date\").min().over(\"series_id\").alias(\"min_date\"),\n",
    "                pl.col(\"date\").max().over(\"series_id\").alias(\"max_date\"),\n",
    "            ).with_columns(\n",
    "                pl.col(event_pred_col)\n",
    "                * (\n",
    "                    pl.lit(1.0)\n",
    "                    - (\n",
    "                        pl.lit(later_date_max_sub_rate)\n",
    "                        * (\n",
    "                            (pl.col(\"date\") - pl.col(\"min_date\")).dt.days().cast(float)\n",
    "                            / ((pl.col(\"max_date\") - pl.col(\"min_date\")).dt.days().cast(float) + 1.0)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        max_event_per_series = event_rate if isinstance(event_rate, int) else int(len(minute_pred_df) * event_rate)\n",
    "\n",
    "        # series_id, chunk_id, step でソート\n",
    "        minute_pred_df = minute_pred_df.sort([\"series_id\", \"chunk_id\", \"step\"])\n",
    "\n",
    "        # 1. 期待値の計算\n",
    "        # 1.1 左側を計算 (同じindexの予測を含む左側を計算)\n",
    "        \"\"\"\n",
    "        以下をそれぞれ計算する\n",
    "        - 15秒(3step)から1分おき(長さN)での候補地点での期待値: stepは 3, 15, 27, 39, 51, 63, 75, 87, 99, 111, ..., (N-1)*12+3\n",
    "        - 45秒(9step)から1分おき(長さN)での候補地点での期待値: stepは 9, 21, 33, 45, 57, 69, 81, 93, 105, 117, ..., (N-1)*12+9\n",
    "        計算は左側の予測地点の数と、右側の予測地点の数\n",
    "        \"\"\"\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        / match_sums[i]\n",
    "                    )\n",
    "                    for i, window in enumerate(high_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_left_expectation_plus_3step\"),\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        / match_sums[i]\n",
    "                    )\n",
    "                    for i, window in enumerate(low_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_left_expectation_plus_9step\"),\n",
    "        )\n",
    "\n",
    "        # 1.2 右側を計算(同じindexの予測を含まない右側を計算。逆順にして一個ずらしrolling_sumを取る必要がある）\n",
    "        minute_pred_df = minute_pred_df.reverse()\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .shift(1)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        .fill_null(0)\n",
    "                        / match_sums[i]\n",
    "                    )\n",
    "                    for i, window in enumerate(low_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_right_expectation_plus_3step\"),\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .shift(1)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        .fill_null(0)\n",
    "                        / match_sums[i]\n",
    "                    )\n",
    "                    for i, window in enumerate(high_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_right_expectation_plus_9step\"),\n",
    "        )\n",
    "        minute_pred_df = minute_pred_df.reverse()\n",
    "\n",
    "        # 合計の期待値計算\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            (pl.col(f\"{event}_left_expectation_plus_3step\") + pl.col(f\"{event}_right_expectation_plus_3step\")).alias(\n",
    "                f\"{event}_expectation_sum_3step\"\n",
    "            ),\n",
    "            (pl.col(f\"{event}_left_expectation_plus_9step\") + pl.col(f\"{event}_right_expectation_plus_9step\")).alias(\n",
    "                f\"{event}_expectation_sum_9step\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # print(display(minute_pred_df))\n",
    "\n",
    "        # 3. 最大値の取得 & 期待値の割引\n",
    "        \"\"\"\n",
    "        各予測地点の power を管理する。powerは以下の11種類\n",
    "        0: その予測地点が影響を与える範囲は無い\n",
    "        1: その予測地点が影響を与える範囲は左右1つ(1min)\n",
    "        2: その予測地点が影響を与える範囲は左右3つ\n",
    "        ︙\n",
    "        10: 左右30(step 0~360)\n",
    "\n",
    "        event を作るたびに、eventからtolerance内にある予測地点のpowerを下げる。\n",
    "        その際に予測地点からtolerance内にある、eventがあったところも含めた候補地点の期待値を割り引く。\n",
    "        \"\"\"\n",
    "        for (series_id,date), series_df in tqdm(\n",
    "            minute_pred_df.select(\n",
    "                [\n",
    "                    \"series_id\",\n",
    "                    \"chunk_id\",\n",
    "                    \"step\",\n",
    "                    event_pred_col,\n",
    "                    f\"{event}_expectation_sum_3step\",\n",
    "                    f\"{event}_expectation_sum_9step\",\n",
    "                    \"date\"\n",
    "                ]\n",
    "            ).group_by([\"series_id\", \"date\"]),\n",
    "            desc=f\"detect {event} peaks\",\n",
    "            leave=False,\n",
    "            total=len(minute_pred_df[[\"series_id\", \"date\"]].unique()),\n",
    "        ):\n",
    "            # chunkごとの id の最大最小を計算\n",
    "            series_df = series_df.with_row_count().with_columns(\n",
    "                pl.col(\"row_nr\").max().over([\"chunk_id\", \"date\"]).alias(\"max_id_in_chunk\"),\n",
    "                pl.col(\"row_nr\").min().over([\"chunk_id\", \"date\"]).alias(\"min_id_in_chunk\"),\n",
    "            )\n",
    "\n",
    "            preds = series_df[event_pred_col].to_numpy()\n",
    "            expectation_sum_3step = series_df[f\"{event}_expectation_sum_3step\"].to_numpy(writable=True)\n",
    "            expectation_sum_9step = series_df[f\"{event}_expectation_sum_9step\"].to_numpy(writable=True)\n",
    "            steps = series_df[f\"step\"].to_numpy(writable=True)\n",
    "            step_id_mins = series_df[\"min_id_in_chunk\"].to_numpy(writable=True)\n",
    "            step_id_maxs = series_df[\"max_id_in_chunk\"].to_numpy(writable=True) + 1\n",
    "            powers = np.ones(len(expectation_sum_3step), dtype=np.int32) * 10\n",
    "\n",
    "            result_steps, result_scores = detect_events_for_serie(\n",
    "                height,\n",
    "                max_event_per_series,\n",
    "                high_match_nums,\n",
    "                low_match_nums,\n",
    "                match_sums,\n",
    "                steps,\n",
    "                step_id_mins,\n",
    "                step_id_maxs,\n",
    "                preds,\n",
    "                expectation_sum_3step,\n",
    "                expectation_sum_9step,\n",
    "                powers,\n",
    "            )\n",
    "\n",
    "            for i in range(len(result_steps)):\n",
    "                result_events_records.append(\n",
    "                    {\n",
    "                        \"series_id\": series_id,\n",
    "                        \"step\": result_steps[i],\n",
    "                        \"event\": event,\n",
    "                        \"score\": result_scores[i],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # print(expectation_sum_3step[max_step_index], expectation_sum_9step[max_step_index])\n",
    "    \n",
    "    if len(result_events_records) == 0:  # 一つも予測がない場合はdummyを入れる\n",
    "        result_events_records.append(\n",
    "            {\n",
    "                \"series_id\": series_id,\n",
    "                \"step\": 0,\n",
    "                \"event\": \"onset\",\n",
    "                \"score\": 0,\n",
    "            }\n",
    "        )\n",
    "    sub_df = pl.DataFrame(result_events_records).sort(by=[\"series_id\", \"step\"])\n",
    "    row_ids = pl.Series(name=\"row_id\", values=np.arange(len(sub_df)))\n",
    "    sub_df = sub_df.with_columns(row_ids).select([\"row_id\", \"series_id\", \"step\", \"event\", \"score\"])\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def detect_events_for_serie(\n",
    "    height,\n",
    "    max_event_per_series,\n",
    "    high_match_nums,\n",
    "    low_match_nums,\n",
    "    match_sums,\n",
    "    steps,\n",
    "    step_id_mins,\n",
    "    step_id_maxs,\n",
    "    preds,\n",
    "    expectation_sum_3step,\n",
    "    expectation_sum_9step,\n",
    "    powers,\n",
    "):\n",
    "    result_steps = []\n",
    "    result_scores = []\n",
    "    for _ in range(max_event_per_series):  # 高い順に最大max_event_per_series個のeventを決定\n",
    "        # 3.1 最大値の取得\n",
    "        # 合計の期待値が最大のstepを取得\n",
    "        max_step3 = expectation_sum_3step.argmax()\n",
    "        max_score3 = expectation_sum_3step[max_step3]\n",
    "        max_step9 = expectation_sum_9step.argmax()\n",
    "        max_score9 = expectation_sum_9step[max_step9]\n",
    "        if max_score3 > max_score9:\n",
    "            # print('max_score3')\n",
    "            left_nums = high_match_nums\n",
    "            right_nums = low_match_nums\n",
    "            max_step_index = max_step3\n",
    "            max_score = max_score3\n",
    "            if max_score < height:  # 閾値以下なら終了\n",
    "                break\n",
    "            result_steps.append(steps[max_step_index] + 3)\n",
    "            result_scores.append(max_score)\n",
    "        else:\n",
    "            # print('max_score9')\n",
    "            left_nums = low_match_nums\n",
    "            right_nums = high_match_nums\n",
    "            max_step_index = max_step9\n",
    "            max_score = max_score9\n",
    "            if max_score < height:  # 閾値以下なら終了\n",
    "                break\n",
    "            result_steps.append(steps[max_step_index] + 9)\n",
    "            result_scores.append(max_score)\n",
    "        # print(f\"max_step_index:{max_step_index}, max_score:{max_score}\")\n",
    "\n",
    "        # 3.2 期待値の割引\n",
    "        \"\"\"\n",
    "        各予測地点のpowerを修正するとともに、候補地点の期待値を割引く。\n",
    "        powerが pi まで小さくなることによってその予測値が影響を与える範囲が狭くなる。\n",
    "        つまり狭くなって範囲から抜けた expectation_sum の値が、その予測値の値*重みの分だけ小さくなる\n",
    "        \"\"\"\n",
    "        # 3.2.1 まずはpowerを修正するstepの候補を探す\n",
    "        target_step_powers = []  # (target_step, pred, base_power, power, step_min, step_max)のリスト\n",
    "        for pi in range(0, 10):\n",
    "            # 左側\n",
    "            for l_diff in range(left_nums[pi], left_nums[pi + 1]):\n",
    "                target_step_index = max_step_index - l_diff\n",
    "                if target_step_index < 0:\n",
    "                    break\n",
    "                pred = preds[target_step_index]\n",
    "                base_power = powers[target_step_index]\n",
    "                if base_power > pi:  # power が小さくなる場合のみ修正\n",
    "                    target_step_powers.append(\n",
    "                        (\n",
    "                            target_step_index,\n",
    "                            pred,\n",
    "                            base_power,\n",
    "                            pi,\n",
    "                            step_id_mins[target_step_index],\n",
    "                            step_id_maxs[target_step_index],\n",
    "                        )\n",
    "                    )\n",
    "            # 右側\n",
    "            for r_diff in range(right_nums[pi] + 1, right_nums[pi + 1] + 1):  # 自分自身と同じindexは含めない\n",
    "                target_step_index = max_step_index + r_diff\n",
    "                if target_step_index >= len(powers):\n",
    "                    break\n",
    "                pred = preds[target_step_index]\n",
    "                base_power = powers[target_step_index]\n",
    "                if base_power > pi:\n",
    "                    target_step_powers.append(\n",
    "                        (\n",
    "                            target_step_index,\n",
    "                            pred,\n",
    "                            base_power,\n",
    "                            pi,\n",
    "                            step_id_mins[target_step_index],\n",
    "                            step_id_maxs[target_step_index],\n",
    "                        )\n",
    "                    )\n",
    "        # print('target_step_powers', target_step_powers)\n",
    "\n",
    "        # 3.2.2 対象となる step の power を修正するとともに期待値を割り引く\n",
    "        \"\"\"\n",
    "        予測地点のpowerを下げるとともに、関連する候補地点の期待値を修正する。\n",
    "        検出したeventから遠い予測地点の場合は、予測地点に近い候補地点であっても期待値はその分割り引かれる。\n",
    "        3stepの修正をする時は target_stepから左側が low_match_nums, 右側が high_match_nums\n",
    "        9stepの修正をする時は target_stepから左側が high_match_nums, 右側が low_match_nums\n",
    "        だんだんと内側のみがのこるように修正する。\n",
    "\n",
    "        - powerが 10 → 8 になるケースは左右1~30個に影響を及ぼしていたものが、左右の1~20個に影響を及ぼすようになる。また、powerが2個減った分全体の期待値も割り引かれる\n",
    "        - powerが 10 → 5 になるケースは左右1~30個に影響を及ぼしていたものが、左右の1~12(13)個に影響を及ぼすようになる\n",
    "        - powerが 8 → 7 になるケースは左右1~20個に影響を及ぼしていたものが、左右の1~15個に影響を及ぼすようになる\n",
    "        \"\"\"\n",
    "        for si, pred, base_power, power, step_min, step_max in target_step_powers:\n",
    "            # print(f\"si:{si}, pred:{pred}, base_power:{base_power}, power:{power}\")\n",
    "            powers[si] = power\n",
    "            # 中心ほど重みが強いので power ごとに処理\n",
    "            for pi in range(base_power, power, -1):  # base_powerからpowerに減らしていくことで予測値の外側から削る\n",
    "                # 3step\n",
    "                left_diff_max = low_match_nums[pi]\n",
    "                right_diff_max = high_match_nums[pi]\n",
    "                expectation_sum_3step[max(si - left_diff_max, step_min) : min(si + right_diff_max, step_max)] -= (\n",
    "                    pred / match_sums[pi - 1]\n",
    "                )\n",
    "                \"\"\"\n",
    "                if ((si-left_diff_max <= max_step_index) and (max_step_index < si+right_diff_max)):\n",
    "                    print(f'3step pi:{pi}')\n",
    "                    print(f\"max_step_index:{max_step_index}, si:{si}, left_diff_max:{left_diff_max}, right_diff_max:{right_diff_max}\") \n",
    "                    print(\"[\", si-left_diff_max, si+right_diff_max, \")\")\n",
    "                    print(f\"power: {pi}→{pi-1}, pred:{pred}\")\n",
    "                    print()\n",
    "                \"\"\"\n",
    "                # 9step\n",
    "                left_diff_max = high_match_nums[pi]\n",
    "                right_diff_max = low_match_nums[pi]\n",
    "                expectation_sum_9step[max(si - left_diff_max, step_min) : min(si + right_diff_max, step_max)] -= (\n",
    "                    pred / match_sums[pi - 1]\n",
    "                )\n",
    "                \"\"\"\n",
    "                if ((si-left_diff_max <= max_step_index) and (max_step_index < si+right_diff_max)):\n",
    "                    print(f'9step pi:{pi}')\n",
    "                    print(f\"max_step_index:{max_step_index}, si:{si}, left_diff_max:{left_diff_max}, right_diff_max:{right_diff_max}\") \n",
    "                    print(\"[\", si-left_diff_max, si+right_diff_max, \")\")\n",
    "                    print(f\"power: {pi}→{pi-1}, pred:{pred}\")\n",
    "                    print()\n",
    "                \"\"\"\n",
    "    return result_steps, result_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42b24405-9ad5-4c82-87e7-3fa98f5d8f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8319978382532234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 一旦 series1つでのスコア計算テスト\n",
    "\"\"\"\n",
    "\"efbfc4526d58\"\n",
    "\"6ca4f4fca6a2\"\n",
    "\"a88088855de5\"\n",
    "\"3be2f86c3e45\"\n",
    "\"e30cb792a2bc\"\n",
    "\"18b61dd5aae8\"\n",
    "\"e6ddbaaf0639\"\n",
    "\"9a340507e36a\"\n",
    "\"\"\"\n",
    "# テストのために一つのseriesに絞る\n",
    "series_id = \"fe90110788d2\" #\"99b829cbad2d\"\n",
    "series_df = pred_df.filter(pl.col('series_id')==series_id)\n",
    "series_event_df = event_df.filter(pl.col('series_id')==series_id)\n",
    "\n",
    "sub_df= post_process_from_2nd(\n",
    "    series_df,\n",
    "    later_date_max_sub_rate=None,\n",
    ")\n",
    "\n",
    "\n",
    "score = event_detection_ap(\n",
    "    series_event_df.to_pandas(),\n",
    "    sub_df.to_pandas(),\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80bce756-94fd-4f25-9d6f-ada2421671a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/6112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/6109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3.8GB(+0.3GB):58.5sec] process \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8336143618853676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3.6GB(-0.2GB):13.1sec] event detection ap \n"
     ]
    }
   ],
   "source": [
    "with trace('process'):\n",
    "    sub_df = post_process_from_2nd(\n",
    "        pred_df,\n",
    "        later_date_max_sub_rate=None,\n",
    "    )\n",
    "    print(len(sub_df))\n",
    "\n",
    "\n",
    "with trace('event detection ap'):\n",
    "    score, table = event_detection_ap(\n",
    "        event_df.to_pandas(),\n",
    "        sub_df.to_pandas(),\n",
    "        with_table=True\n",
    "    )\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2074e95-b4a6-4afa-918c-f86077d1916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 383960: 0.8328379631952278"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f7a93-fce0-47e3-bba8-336749bffe68",
   "metadata": {},
   "source": [
    "## date ごとに chokudaiサーチする\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80c90d59-aeb6-42c9-b7a3-673af8647479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "from numba import jit\n",
    "import copy\n",
    "\n",
    "\n",
    "def chokudai_search_from_2nd(\n",
    "    pred_df,\n",
    "    max_event_per_date: int = 50,\n",
    "    event2col: dict[str, str] = {\"onset\": \"stacking_prediction_onset\", \"wakeup\": \"stacking_prediction_wakeup\"},\n",
    "    event2offset: dict[str, str] = {\"onset\": \"5h\", \"wakeup\": \"0h\"},\n",
    "    use_daily_norm: bool = True,\n",
    "    daily_score_offset=10,\n",
    "    later_date_max_sub_rate: float | None = 0.05,\n",
    "    time_limit: int = 1,\n",
    "    candidate_num: int = 3,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    1分ごとの予測値を用いてイベントを検出する\n",
    "    用語\n",
    "    - 予測地点: 2段目のモデルによって得られた1分毎の予測位置\n",
    "    - 候補地点: event の候補となる 15秒 or 45秒始まりの30秒間隔の位置\n",
    "\n",
    "    Args:\n",
    "        pred_df (pl.DataFrame): timestamp 込み\n",
    "        event_rate (int | float, optional): [0,1) の値であれば1分間に何回イベントが起こるか。intの場合はseries_idごとに同じイベント数を検出。 Defaults to 0.005.\n",
    "        height (float, optional): 候補地点の期待値がこの値を下回ったら終了。 Defaults to 0.1.\n",
    "        time_limit (int, optional): chokudai search １回あたりの時間制限。 Defaults to 1.\n",
    "        event2col (dict[str, str], optional): event名と予測値のカラム名の対応。 Defaults to {\"onset\": \"stacking_prediction_onset\", \"wakeup\": \"stacking_prediction_wakeup\"}.\n",
    "        weight_rate (float | None, optional): 遠くの予測値の期待値を割り引く際の重み。Noneの場合は重みを1とする。1/weight_rate 倍ずつ遠くの予測値の重みが小さくなっていく。 Defaults to None.\n",
    "        use_daily_norm (bool, optional): 一日ごとに予測値を正規化するかどうか。 Defaults to False.\n",
    "        daily_score_offset (float, optional): 正規化の際のoffset。 Defaults to 1.0.\n",
    "        later_date_max_sub_rate (float | None, optional): 日付が古いほど予測値を割り引く際の最大割引率。Noneの場合は割引しない。 Defaults to None.\n",
    "    Returns:\n",
    "        event_df (pl.DataFrame): row_id, series_id, step, event, score をカラムに持つ。\n",
    "    \"\"\"\n",
    "    high_match_nums = (0, 1, 3, 5, 8, 10, 13, 15, 20, 25, 30)\n",
    "    low_match_nums = (0, 1, 3, 5, 7, 10, 12, 15, 20, 25, 30)\n",
    "    result_events_records = []\n",
    "\n",
    "    # event ごとに処理\n",
    "    for event, event_pred_col in event2col.items():\n",
    "        \"\"\"\n",
    "        元の系列の予測地点(長さN): 0, 12, 24, 36, 48, 60, 72, 84, 96, 108, ..., (N-1)*12\n",
    "        15秒から30秒おきのevent候補地点(長さ2N): 3, 9, 15, 21, 27, 33, 39, 45, 51, 57, ..., (N-1)*12+3, (N-1)*12+9\n",
    "            - 15秒(3step)から1分おき(長さN): 3, 15, 27, 39, 51, 63, 75, 87, 99, 111, ..., (N-1)*12+3\n",
    "                - 左の個数 {12: 1, 36: 3, 60: 5, 90: *8*, 120: 10, 150: *13*, 180: 15, 240: 20, 300: 25, 360: 30} high_match_nums\n",
    "                - 右の個数 {12: 1, 36: 3, 60: 5, 90: *7*, 120: 10, 150: *12*, 180: 15, 240: 20, 300: 25, 360: 30} low_match_nums\n",
    "            - 45秒(9step)から1分おき(長さN): 9, 21, 33, 45, 57, 69, 81, 93, 105, 117, ..., (N-1)*12+9\n",
    "                - 左の個数 {12: 1, 36: 3, 60: 5, 90: *7*, 120: 10, 150: *12*, 180: 15, 240: 20, 300: 25, 360: 30} low_match_nums\n",
    "                - 右の個数 {12: 1, 36: 3, 60: 5, 90: *8*, 120: 10, 150: *13*, 180: 15, 240: 20, 300: 25, 360: 30} high_match_nums       \n",
    "        \"\"\"\n",
    "\n",
    "        # series内でのindexを振り、chunk内での最大と最小を計算\n",
    "        minute_pred_df = pred_df.filter(pl.col(\"timestamp\").is_not_null())\n",
    "\n",
    "        if use_daily_norm:\n",
    "            minute_pred_df = (\n",
    "                minute_pred_df.with_columns(\n",
    "                    pl.col(\"timestamp\").dt.offset_by(event2offset[event]).dt.date().alias(\"date\")\n",
    "                )\n",
    "                .with_columns(pl.col(event_pred_col).sum().over([\"series_id\", \"date\"]).alias(\"date_sum\"))\n",
    "                .with_columns(\n",
    "                    pl.col(event_pred_col) / (pl.col(\"date_sum\") + (1 / (daily_score_offset + pl.col(\"date_sum\"))))\n",
    "                )\n",
    "            )\n",
    "        if later_date_max_sub_rate is not None:\n",
    "            minute_pred_df = minute_pred_df.with_columns(\n",
    "                pl.col(\"date\").min().over(\"series_id\").alias(\"min_date\"),\n",
    "                pl.col(\"date\").max().over(\"series_id\").alias(\"max_date\"),\n",
    "            ).with_columns(\n",
    "                pl.col(event_pred_col)\n",
    "                * (\n",
    "                    pl.lit(1.0)\n",
    "                    - (\n",
    "                        pl.lit(later_date_max_sub_rate)\n",
    "                        * (\n",
    "                            (pl.col(\"date\") - pl.col(\"min_date\")).dt.days().cast(float)\n",
    "                            / ((pl.col(\"max_date\") - pl.col(\"min_date\")).dt.days().cast(float) + 1.0)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # series_id, chunk_id, step でソート\n",
    "        minute_pred_df = minute_pred_df.sort([\"series_id\", \"chunk_id\", \"step\"])\n",
    "\n",
    "        # 1. 期待値の計算\n",
    "        # 1.1 左側を計算 (同じindexの予測を含む左側を計算)\n",
    "        \"\"\"\n",
    "        以下をそれぞれ計算する\n",
    "        - 15秒(3step)から1分おき(長さN)での候補地点での期待値: stepは 3, 15, 27, 39, 51, 63, 75, 87, 99, 111, ..., (N-1)*12+3\n",
    "        - 45秒(9step)から1分おき(長さN)での候補地点での期待値: stepは 9, 21, 33, 45, 57, 69, 81, 93, 105, 117, ..., (N-1)*12+9\n",
    "        計算は左側の予測地点の数と、右側の予測地点の数\n",
    "        \"\"\"\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                    )\n",
    "                    for i, window in enumerate(high_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_left_expectation_plus_3step\"),\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                    )\n",
    "                    for i, window in enumerate(low_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_left_expectation_plus_9step\"),\n",
    "        )\n",
    "\n",
    "        # 1.2 右側を計算(同じindexの予測を含まない右側を計算。逆順にして一個ずらしrolling_sumを取る必要がある）\n",
    "        minute_pred_df = minute_pred_df.reverse()\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .shift(1)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        .fill_null(0)\n",
    "                    )\n",
    "                    for i, window in enumerate(low_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_right_expectation_plus_3step\"),\n",
    "            pl.sum_horizontal(\n",
    "                [\n",
    "                    (\n",
    "                        pl.col(event_pred_col)\n",
    "                        .shift(1)\n",
    "                        .rolling_sum(window_size=window, center=False, min_periods=1)\n",
    "                        .over([\"series_id\", \"chunk_id\"])\n",
    "                        .fill_null(0)\n",
    "                    )\n",
    "                    for i, window in enumerate(high_match_nums[1:])\n",
    "                ]\n",
    "            ).alias(f\"{event}_right_expectation_plus_9step\"),\n",
    "        )\n",
    "        minute_pred_df = minute_pred_df.reverse()\n",
    "\n",
    "        # 合計の期待値計算\n",
    "        minute_pred_df = minute_pred_df.with_columns(\n",
    "            (pl.col(f\"{event}_left_expectation_plus_3step\") + pl.col(f\"{event}_right_expectation_plus_3step\")).alias(\n",
    "                f\"{event}_expectation_sum_3step\"\n",
    "            ),\n",
    "            (pl.col(f\"{event}_left_expectation_plus_9step\") + pl.col(f\"{event}_right_expectation_plus_9step\")).alias(\n",
    "                f\"{event}_expectation_sum_9step\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # print(display(minute_pred_df))\n",
    "        # 3. date ごとに期待値が最大となるように event を検出\n",
    "        for (series_id, date), series_df in tqdm(\n",
    "            minute_pred_df.select(\n",
    "                [\n",
    "                    \"series_id\",\n",
    "                    \"chunk_id\",\n",
    "                    \"step\",\n",
    "                    event_pred_col,\n",
    "                    f\"{event}_expectation_sum_3step\",\n",
    "                    f\"{event}_expectation_sum_9step\",\n",
    "                    \"date\",\n",
    "                ]\n",
    "            ).group_by([\"series_id\", \"date\"]),\n",
    "            desc=f\"detect {event} peaks\",\n",
    "            leave=False,\n",
    "            total=len(minute_pred_df[[\"series_id\", \"date\"]].unique()),\n",
    "        ):\n",
    "            # chunkごとの id の最大最小を計算\n",
    "            series_df = series_df.with_row_count().with_columns(\n",
    "                pl.col(\"row_nr\").max().over([\"chunk_id\", \"date\"]).alias(\"max_id_in_chunk\"),\n",
    "                pl.col(\"row_nr\").min().over([\"chunk_id\", \"date\"]).alias(\"min_id_in_chunk\"),\n",
    "            )\n",
    "\n",
    "            preds = series_df[event_pred_col].to_numpy()\n",
    "            expectation_sum_3step = series_df[f\"{event}_expectation_sum_3step\"].to_numpy(writable=True)\n",
    "            expectation_sum_9step = series_df[f\"{event}_expectation_sum_9step\"].to_numpy(writable=True)\n",
    "            steps = series_df[f\"step\"].to_numpy(writable=True)\n",
    "            step_id_mins = series_df[\"min_id_in_chunk\"].to_numpy(writable=True)\n",
    "            step_id_maxs = series_df[\"max_id_in_chunk\"].to_numpy(writable=True) + 1\n",
    "\n",
    "            result_steps, result_scores = chokudai_search_for_one_day(\n",
    "                steps,\n",
    "                preds,\n",
    "                step_id_mins,\n",
    "                step_id_maxs,\n",
    "                expectation_sum_3step,\n",
    "                expectation_sum_9step,\n",
    "                time_limit=time_limit,\n",
    "                max_event=max_event_per_date,\n",
    "                chokudai_width=1,\n",
    "                candidate_num=candidate_num,\n",
    "            )\n",
    "\n",
    "            for i in range(len(result_steps)):\n",
    "                result_events_records.append(\n",
    "                    {\n",
    "                        \"series_id\": series_id,\n",
    "                        \"step\": result_steps[i],\n",
    "                        \"event\": event,\n",
    "                        \"score\": result_scores[i],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # print(expectation_sum_3step[max_step_index], expectation_sum_9step[max_step_index])\n",
    "\n",
    "    if len(result_events_records) == 0:  # 一つも予測がない場合はdummyを入れる\n",
    "        result_events_records.append(\n",
    "            {\n",
    "                \"series_id\": series_id,\n",
    "                \"step\": 0,\n",
    "                \"event\": \"onset\",\n",
    "                \"score\": 0,\n",
    "            }\n",
    "        )\n",
    "    sub_df = pl.DataFrame(result_events_records).sort(by=[\"series_id\", \"step\"])\n",
    "    row_ids = pl.Series(name=\"row_id\", values=np.arange(len(sub_df)))\n",
    "    sub_df = sub_df.with_columns(row_ids).select([\"row_id\", \"series_id\", \"step\", \"event\", \"score\"])\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "from numba import jit, objmode\n",
    "\n",
    "# chokudai search\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sum_score: float,\n",
    "        steps: list[int],\n",
    "        scores: list[float],\n",
    "        expectation_sum_3step: np.ndarray,\n",
    "        expectation_sum_9step: np.ndarray,\n",
    "        powers: np.ndarray,\n",
    "    ):\n",
    "        self.sum_score = sum_score\n",
    "        self.steps = steps\n",
    "        self.scores = scores\n",
    "        self.expectation_sum_3step = expectation_sum_3step\n",
    "        self.expectation_sum_9step = expectation_sum_9step\n",
    "        self.powers = powers\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.sum_score > other.sum_score\n",
    "\n",
    "    def get_k_best_next_states(\n",
    "        self,\n",
    "        steps,\n",
    "        preds,\n",
    "        step_id_mins,\n",
    "        step_id_maxs,\n",
    "        candidate_num: int = 3,\n",
    "    ) -> list[\"State\"]:\n",
    "        \"\"\"\n",
    "        このstateからcandidate_num個の次のstateを返す\n",
    "        \"\"\"\n",
    "        next_states = []\n",
    "        top_k_step_id_3 = np.argpartition(-self.expectation_sum_3step, candidate_num)[:candidate_num]\n",
    "        top_k_step_id_9 = np.argpartition(-self.expectation_sum_9step, candidate_num)[:candidate_num]\n",
    "        max_step_ids = []\n",
    "\n",
    "        # 候補を追加 (expectation_sum_3/9step, powers だけはコピーはされるが更新はされていない)\n",
    "        for step_id_3 in top_k_step_id_3:\n",
    "            score = self.expectation_sum_3step[step_id_3]\n",
    "            new_state = copy.deepcopy(self)\n",
    "            new_state.sum_score += score\n",
    "            new_state.steps.append(steps[step_id_3] + 3)\n",
    "            new_state.scores.append(score)\n",
    "            next_states.append(new_state)\n",
    "            max_step_ids.append(step_id_3)\n",
    "        for step_id_9 in top_k_step_id_9:\n",
    "            score = self.expectation_sum_9step[step_id_9]\n",
    "            new_state = copy.deepcopy(self)\n",
    "            new_state.sum_score += score\n",
    "            new_state.steps.append(steps[step_id_9] + 9)\n",
    "            new_state.scores.append(score)\n",
    "            next_states.append(new_state)\n",
    "            max_step_ids.append(step_id_9)\n",
    "\n",
    "        # 期待値の割引(expectation_sum を更新)\n",
    "        for i, next_state in enumerate(next_states):\n",
    "            update_state(\n",
    "                preds,\n",
    "                step_id_mins,\n",
    "                step_id_maxs,\n",
    "                max_step_ids[i],\n",
    "                next_state.steps[-1],\n",
    "                next_state.expectation_sum_3step,\n",
    "                next_state.expectation_sum_9step,\n",
    "                next_state.powers,\n",
    "            )\n",
    "\n",
    "        return next_states\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def update_state(\n",
    "    preds,\n",
    "    step_id_mins,\n",
    "    step_id_maxs,\n",
    "    max_step_index,\n",
    "    step,\n",
    "    expectation_sum_3step,\n",
    "    expectation_sum_9step,\n",
    "    powers,\n",
    "):\n",
    "    high_match_nums = (0, 1, 3, 5, 8, 10, 13, 15, 20, 25, 30)\n",
    "    low_match_nums = (0, 1, 3, 5, 7, 10, 12, 15, 20, 25, 30)\n",
    "\n",
    "    if step % 12 == 3:\n",
    "        left_nums = high_match_nums\n",
    "        right_nums = low_match_nums\n",
    "    else:\n",
    "        left_nums = low_match_nums\n",
    "        right_nums = high_match_nums\n",
    "\n",
    "    target_step_powers = []  # (target_step, pred, base_power, new_power, step_min, step_max)のリスト\n",
    "    for pi in range(0, 10):\n",
    "        # 左側\n",
    "        for l_diff in range(left_nums[pi], left_nums[pi + 1]):\n",
    "            target_step_index = max_step_index - l_diff\n",
    "            if target_step_index < 0:\n",
    "                break\n",
    "            pred = preds[target_step_index]\n",
    "            base_power = powers[target_step_index]\n",
    "            if base_power > pi:  # power が小さくなる場合のみ修正\n",
    "                target_step_powers.append(\n",
    "                    (\n",
    "                        target_step_index,\n",
    "                        pred,\n",
    "                        base_power,\n",
    "                        pi,\n",
    "                        step_id_mins[target_step_index],\n",
    "                        step_id_maxs[target_step_index],\n",
    "                    )\n",
    "                )\n",
    "        # 右側\n",
    "        for r_diff in range(right_nums[pi] + 1, right_nums[pi + 1] + 1):  # 自分自身と同じindexは含めない\n",
    "            target_step_index = max_step_index + r_diff\n",
    "            if target_step_index >= len(powers):\n",
    "                break\n",
    "            pred = preds[target_step_index]\n",
    "            base_power = powers[target_step_index]\n",
    "            if base_power > pi:\n",
    "                target_step_powers.append(\n",
    "                    (\n",
    "                        target_step_index,\n",
    "                        pred,\n",
    "                        base_power,\n",
    "                        pi,\n",
    "                        step_id_mins[target_step_index],\n",
    "                        step_id_maxs[target_step_index],\n",
    "                    )\n",
    "                )\n",
    "    for si, pred, base_power, new_power, step_min, step_max in target_step_powers:\n",
    "        # print(f\"si:{si}, pred:{pred}, base_power:{base_power}, power:{power}\")\n",
    "        powers[si] = new_power\n",
    "        # 中心ほど重みが強いので power ごとに処理\n",
    "        for pi in range(base_power, new_power, -1):  # base_powerからpowerに減らしていくことで予測値の外側から削る\n",
    "            # 3step\n",
    "            left_diff_max = low_match_nums[pi]\n",
    "            right_diff_max = high_match_nums[pi]\n",
    "            expectation_sum_3step[max(si - left_diff_max, step_min) : min(si + right_diff_max, step_max)] -= pred\n",
    "            # 9step\n",
    "            left_diff_max = high_match_nums[pi]\n",
    "            right_diff_max = low_match_nums[pi]\n",
    "            expectation_sum_9step[max(si - left_diff_max, step_min) : min(si + right_diff_max, step_max)] -= pred\n",
    "\n",
    "\n",
    "def chokudai_search_for_one_day(\n",
    "    steps,\n",
    "    preds,\n",
    "    step_id_mins,\n",
    "    step_id_maxs,\n",
    "    expectation_sum_3step: np.ndarray,\n",
    "    expectation_sum_9step: np.ndarray,\n",
    "    time_limit: int,\n",
    "    max_event: int,\n",
    "    chokudai_width: int = 1,\n",
    "    candidate_num: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    一日分のデータに対してchokudai searchを行う\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: (steps, scores) のタプル\n",
    "    \"\"\"\n",
    "    max_turn = max_event\n",
    "\n",
    "    # 初期化\n",
    "    heap_states = [[] for _ in range(max_turn + 1)]\n",
    "    heap_states[0].append(\n",
    "        State(\n",
    "            0.0,\n",
    "            [],\n",
    "            [],\n",
    "            expectation_sum_3step.copy(),\n",
    "            expectation_sum_9step.copy(),\n",
    "            powers=np.ones(len(expectation_sum_3step), dtype=np.int32) * 10,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # time_limitの間だけ実行する\n",
    "    start_time = time.time()  # 計測開始\n",
    "    while time.time() - start_time < time_limit:\n",
    "        for turn_i in range(max_turn):\n",
    "            for _ in range(chokudai_width):\n",
    "                if len(heap_states[turn_i]) == 0:\n",
    "                    break\n",
    "                state = heapq.heappop(heap_states[turn_i])\n",
    "                for next_state in state.get_k_best_next_states(\n",
    "                    steps,\n",
    "                    preds,\n",
    "                    step_id_mins,\n",
    "                    step_id_maxs,\n",
    "                    candidate_num=candidate_num,\n",
    "                ):\n",
    "                    heapq.heappush(heap_states[turn_i + 1], next_state)\n",
    "        #best_state = heapq.nsmallest(1, heap_states[max_turn])[0]\n",
    "        #print(f'best score: {best_state.sum_score}')\n",
    "    #print()\n",
    "\n",
    "    # 最大のスコアを持つものを返す\n",
    "    best_state = heapq.heappop(heap_states[max_turn])\n",
    "\n",
    "    return best_state.steps, best_state.scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d463b8ab-50ff-494d-ab29-15a5761483a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5e816f11f5c3',\n",
       " '5c088d7e916c',\n",
       " '1955d568d987',\n",
       " 'dacc6d652e35',\n",
       " 'a88088855de5',\n",
       " '6d6b9d22d48a',\n",
       " '1d4569cbac0f',\n",
       " 'c908a0ad3e31',\n",
       " 'f564985ab692',\n",
       " 'f2c2436cf7b7']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['series_id'].unique().to_list()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf509f3c-9b92-4066-bdbd-c1ac865eb764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401507437266945\n"
     ]
    }
   ],
   "source": [
    "series_list = ['6d6b9d22d48a',]\n",
    "series_df = pred_df.filter(pl.col(\"series_id\").is_in(series_list))\n",
    "series_event_df = event_df.filter(pl.col(\"series_id\").is_in(series_list))\n",
    "\n",
    "sub_df= post_process_from_2nd(\n",
    "    series_df,\n",
    "    later_date_max_sub_rate=None,\n",
    ")\n",
    "\n",
    "\n",
    "score = event_detection_ap(\n",
    "    series_event_df.to_pandas(),\n",
    "    sub_df.to_pandas(),\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4dbd8be7-b77f-4e27-a8c1-2a918af3ef33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9362345196149853\n"
     ]
    }
   ],
   "source": [
    "sub_df = chokudai_search_from_2nd(\n",
    "    series_df,\n",
    "    later_date_max_sub_rate=None,\n",
    "    time_limit = 2,\n",
    "    candidate_num = 3,\n",
    ")\n",
    "\n",
    "\n",
    "score = event_detection_ap(\n",
    "    series_event_df.to_pandas(),\n",
    "    sub_df.to_pandas(),\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9822efed-098a-411d-8bf8-2c7d36f164a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927862957273945\n"
     ]
    }
   ],
   "source": [
    "sub_df = chokudai_search_from_2nd(\n",
    "    series_df,\n",
    "    later_date_max_sub_rate=None,\n",
    "    time_limit = 5,\n",
    "    candidate_num = 10,\n",
    ")\n",
    "\n",
    "\n",
    "score = event_detection_ap(\n",
    "    series_event_df.to_pandas(),\n",
    "    sub_df.to_pandas(),\n",
    ")\n",
    "\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9de9b38d-feaf-4c2f-a34e-a3a1640f0efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect onset peaks:   0%|          | 0/6112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detect wakeup peaks:   0%|          | 0/6109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3.9GB(+0.3GB):60.6sec] process \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching detections to ground truth events:   0%|          | 0/538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8336143618853676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3.7GB(-0.2GB):12.8sec] event detection ap \n"
     ]
    }
   ],
   "source": [
    "with trace('process'):\n",
    "    sub_df = post_process_from_2nd(\n",
    "        pred_df,\n",
    "        later_date_max_sub_rate=None,\n",
    "    )\n",
    "    print(len(sub_df))\n",
    "\n",
    "\n",
    "with trace('event detection ap'):\n",
    "    score, table = event_detection_ap(\n",
    "        event_df.to_pandas(),\n",
    "        sub_df.to_pandas(),\n",
    "        with_table=True\n",
    "    )\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee5713-f08c-4ddb-a6e5-e8a279c984f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
